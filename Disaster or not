{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utpxFZFA1ugj",
        "outputId": "cd61858f-fa4f-43f4-8114-3fff4de9cbeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "   id keyword place                                              tweet  \\\n",
            "0   1     NaN   NaN  Our Deeds are the Reason of this #earthquake M...   \n",
            "1   4     NaN   NaN             Forest fire near La Ronge Sask. Canada   \n",
            "2   5     NaN   NaN  All residents asked to 'shelter in place' are ...   \n",
            "3   6     NaN   NaN  13,000 people receive #wildfires evacuation or...   \n",
            "4   7     NaN   NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
            "\n",
            "   disaster  \n",
            "0         1  \n",
            "1         1  \n",
            "2         1  \n",
            "3         1  \n",
            "4         1  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7613 entries, 0 to 7612\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        7613 non-null   int64 \n",
            " 1   keyword   7552 non-null   object\n",
            " 2   place     5080 non-null   object\n",
            " 3   tweet     7613 non-null   object\n",
            " 4   disaster  7613 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 297.5+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "url = '/content/test.csv'\n",
        "url = '/content/train.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display first few rows and check columns\n",
        "print(df.head())\n",
        "print(df.info())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'tweet' contains the text data, adjust your code as follows:\n",
        "\n",
        "# Feature selection (assuming 'tweet' and 'disaster' columns are relevant)\n",
        "X = df['tweet']\n",
        "y = df['disaster']\n",
        "\n",
        "# Text Preprocessing\n",
        "# Follow the steps for text preprocessing as shown earlier\n",
        "\n",
        "# Example:\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
        "\n",
        "# Fit and transform the text data\n",
        "X = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# Check the shape of X\n",
        "print(f'X shape after vectorization: {X.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKwFYhNU2V1W",
        "outputId": "af43fe3d-73e8-47ee-d8fc-d02d47eae3fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape after vectorization: (5080, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'tweet' contains the text data, adjust your code as follows:\n",
        "\n",
        "# Feature selection (assuming 'tweet' and 'disaster' columns are relevant)\n",
        "X = df['tweet']\n",
        "y = df['disaster']\n",
        "\n",
        "# Text Preprocessing\n",
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to preprocess text data\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Additional preprocessing steps can be added (e.g., stemming, stopwords removal)\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to 'X'\n",
        "X = X.apply(preprocess_text)\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
        "\n",
        "# Fit and transform the preprocessed text data\n",
        "X = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# Check the shape of X\n",
        "print(f'X shape after vectorization: {X.shape}')\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shapes of train and test sets\n",
        "print(f'Train set - X: {X_train.shape}, y: {y_train.shape}')\n",
        "print(f'Test set - X: {X_test.shape}, y: {y_test.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy8EXL4t28ao",
        "outputId": "1f1cc443-e89d-4a06-ab27-ef571a08c7ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape after vectorization: (5080, 5000)\n",
            "Train set - X: (4064, 5000), y: (4064,)\n",
            "Test set - X: (1016, 5000), y: (1016,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the dataset into training and testing sets (you can adjust the test_size and random_state as needed)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shapes of train and test sets\n",
        "print(f'Train set - X: {X_train.shape}, y: {y_train.shape}')\n",
        "print(f'Test set - X: {X_test.shape}, y: {y_test.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm-7BSuF3Ov8",
        "outputId": "4fe9b3ad-0b42-4365-9768-59330d5b4577"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set - X: (4064, 5000), y: (4064,)\n",
            "Test set - X: (1016, 5000), y: (1016,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the classifier (Logistic Regression is a good starting point)\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Classification report (provides precision, recall, F1-score)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFnOuzcp3QLa",
        "outputId": "34ae1871-32c1-4ff1-8741-0a60a0d29554"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7962598425196851\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.89      0.83       580\n",
            "           1       0.82      0.67      0.74       436\n",
            "\n",
            "    accuracy                           0.80      1016\n",
            "   macro avg       0.80      0.78      0.79      1016\n",
            "weighted avg       0.80      0.80      0.79      1016\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional evaluation metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lcbj8fVb3WFf",
        "outputId": "f11905c6-7ce3-42bd-af81-cd907f6057bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[515  65]\n",
            " [142 294]]\n"
          ]
        }
      ]
    }
  ]
}
